# -*- coding: utf-8 -*-
"""black spot detection

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KOQU7aKICDtaCZJV5tHRHzlAZVZroLNj
"""

import pandas as pd
import geopandas as gpd
from shapely.geometry import Point
from sklearn.cluster import DBSCAN
import numpy as np
import folium

import kagglehub

import os

# use the path returned by kagglehub
data_path = "/root/.cache/kagglehub/datasets/sobhanmoosavi/us-accidents/versions/13"

# list files to check the exact CSV name
print(os.listdir(data_path))

import pandas as pd

# keep only required columns
use_cols = ["ID", "Severity", "Start_Time", "End_Time", "Start_Lat", "Start_Lng",
            "State", "County", "Distance(mi)", "Street", "City", "Zipcode",
            "Temperature(F)", "Wind_Chill(F)", "Humidity(%)", "Visibility(mi)",
            "Wind_Speed(mph)", "Weather_Condition", "Traffic_Calming"]

# correct path from kagglehub
file_path = "/root/.cache/kagglehub/datasets/sobhanmoosavi/us-accidents/versions/13/US_Accidents_March23.csv"

chunksize = 100000   # read in batches
chunks = []

for chunk in pd.read_csv(file_path, usecols=use_cols, chunksize=chunksize):
    chunk = chunk.dropna(subset=["Start_Lat", "Start_Lng"])
    chunks.append(chunk)

df = pd.concat(chunks, ignore_index=True)

print("Final dataset shape:", df.shape)
print(df.head())

df["Start_Time"] = pd.to_datetime(df["Start_Time"], errors="coerce")
df["End_Time"]   = pd.to_datetime(df["End_Time"], errors="coerce")

# Extract useful features from datetime
df["Year"] = df["Start_Time"].dt.year
df["Month"] = df["Start_Time"].dt.month
df["Day"] = df["Start_Time"].dt.day
df["Hour"] = df["Start_Time"].dt.hour
df["DayOfWeek"] = df["Start_Time"].dt.dayofweek  # 0=Monday

# Handle missing values for weather/traffic
df["Weather_Condition"] = df["Weather_Condition"].fillna("Unknown")
df["Traffic_Calming"] = df["Traffic_Calming"].fillna("None")

# Replace extreme outliers (example for wind speed > 150 mph)
df["Wind_Speed(mph)"] = np.where(df["Wind_Speed(mph)"] > 150, np.nan, df["Wind_Speed(mph)"])
df["Wind_Speed(mph)"].fillna(df["Wind_Speed(mph)"].median(), inplace=True)

print("Dataset after preprocessing:", df.shape)
print(df[["Severity","Year","Month","Hour","Weather_Condition","Traffic_Calming"]].head())

from sklearn.cluster import DBSCAN
from geopy.distance import great_circle
import numpy as np

# Prepare lat/lon data
coords = df[["Start_Lat", "Start_Lng"]].values

# DBSCAN clustering
kms_per_radian = 6371.0088  # Earth radius in km
epsilon = 0.5 / kms_per_radian  # 0.5 km radius

db = DBSCAN(eps=epsilon, min_samples=20, algorithm="ball_tree", metric="haversine").fit(np.radians(coords))

df["Cluster"] = db.labels_

# Show top blackspots (clusters with most accidents)
blackspots = df["Cluster"].value_counts().head(10)
print("Top blackspot clusters:")
print(blackspots)

# Sample 100k points to avoid memory crash
sample_df = df.sample(100000, random_state=42)

coords = sample_df[["Start_Lat", "Start_Lng"]].values

from sklearn.cluster import DBSCAN
import numpy as np

# DBSCAN parameters
kms_per_radian = 6371.0088
epsilon = 0.5 / kms_per_radian  # 0.5 km neighborhood

db = DBSCAN(eps=epsilon, min_samples=20, algorithm="ball_tree", metric="haversine").fit(np.radians(coords))
sample_df["Cluster"] = db.labels_

print("Clusters found:", sample_df["Cluster"].nunique())
print(sample_df["Cluster"].value_counts().head())

import folium
from folium.plugins import HeatMap

# 1. Create base map
m = folium.Map(location=[df["Start_Lat"].mean(), df["Start_Lng"].mean()], zoom_start=6)

# 2. Prepare heatmap data (sample for speed)
#    If dataset is huge, sample 50k points
heat_data = df[["Start_Lat", "Start_Lng"]].sample(50000, random_state=42).values.tolist()

# 3. Add HeatMap layer
HeatMap(
    heat_data,
    radius=8,       # size of each point
    blur=15,        # spread of heat
    max_zoom=10,    # max zoom level
).add_to(m)

# 4. Save map
m.save("blackspot_heatmap.html")
print("‚úÖ Heatmap saved ‚Üí blackspot_heatmap.html (open in browser)")

pip install fastapi uvicorn

import pandas as pd
from fastapi import FastAPI

app = FastAPI(title="Road Safety Blackspot API üö¶")

# ---- Load preprocessed accident dataset ----
use_cols = ["ID", "Severity", "Start_Lat", "Start_Lng", "State", "County", "City", "Street"]
file_path = "/root/.cache/kagglehub/datasets/sobhanmoosavi/us-accidents/versions/13/US_Accidents_March23.csv"

# Peek at actual columns in dataset
all_cols = pd.read_csv(file_path, nrows=1).columns.tolist()
available_cols = [col for col in use_cols if col in all_cols]

if not available_cols:
    raise ValueError("‚ùå None of the expected columns found in dataset!")

df = pd.read_csv(file_path, usecols=available_cols, nrows=200000)

# ---- API Endpoints ----
@app.get("/")
def home():
    return {"message": "Road Safety Blackspot API is running üö¶"}

@app.get("/blackspots/")
def get_blackspots(state: str = None, city: str = None):
    """
    Query blackspots by state or city
    Example: /blackspots?state=CA&city=Los Angeles
    """
    if df.empty or not {"City", "Street", "Severity", "ID"}.issubset(df.columns):
        return {"detail": "Data not loaded or processed correctly."}

    data = df.copy()
    if state and "State" in data.columns:
        data = data[data["State"].str.upper() == state.upper()]
    if city and "City" in data.columns:
        data = data[data["City"].str.contains(city, case=False, na=False)]

    summary = (
        data.groupby(["City", "Street"])
        .agg(accidents=("ID", "count"), avg_severity=("Severity", "mean"))
        .reset_index()
        .sort_values("accidents", ascending=False)
        .head(20)
        .to_dict(orient="records")
    )

    return {"blackspots": summary}

import geopandas as gpd

# Drop rows with missing coordinates and keep required columns
subset_df = df[["Start_Lat", "Start_Lng", "Cluster", "State", "City"]].dropna()

# Create GeoDataFrame safely
gdf = gpd.GeoDataFrame(
    subset_df,
    geometry=gpd.points_from_xy(subset_df["Start_Lng"], subset_df["Start_Lat"]),
    crs="EPSG:4326"
)

# Save GeoJSON for visualization
output_file = "blackspot_clusters.geojson"
gdf.to_file(output_file, driver="GeoJSON")

print(f"‚úÖ GeoJSON saved: {output_file}")
print("Total blackspots exported:", len(gdf))

from google.colab import files
files.download("blackspot_heatmap.html")

from google.colab import files
files.download("blackspot_clusters.geojson")



import folium
import geopandas as gpd

# Load the GeoJSON file you created earlier
gdf = gpd.read_file("blackspot_clusters.geojson")

# Create base map centered on dataset
m = folium.Map(location=[gdf.geometry.y.mean(), gdf.geometry.x.mean()], zoom_start=6)

# Plot each blackspot cluster as a line (or point cluster)
for _, row in gdf.iterrows():
    folium.CircleMarker(
        location=[row.geometry.y, row.geometry.x],
        radius=3,
        color="red",
        fill=True,
        fill_opacity=0.6
    ).add_to(m)

# Save output map
m.save("blackspot_map.html")
print("‚úÖ Map saved as blackspot_map.html ‚Üí open in browser")

from google.colab import files
files.download("blackspot_map.html")